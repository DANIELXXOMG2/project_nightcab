"""
Script 05: Procesamiento Escalable con PySpark
==============================================
Descripci√≥n
-----------
Este script carga el dataset maestro en formato Parquet ("./data/processed/master_dataset.parquet"),
asegura que las columnas temporales est√©n en tipo `timestamp` para Spark, realiza agregaciones
escalables (con PySpark) y persiste resultados.

Qu√© hace exactamente:
- Lee el Parquet maestro generado previamente por el pipeline (script 03).
- Verifica/castea columnas de tiempo: `started_at`, `ended_at`, `datetime_hourly` ‚Üí `timestamp`.
- Agregaci√≥n 1: duraci√≥n promedio, desviaci√≥n est√°ndar y total de viajes por hora del d√≠a.
- Agregaci√≥n 2: conteo de viajes por temperatura (redondeada a entero).
- Intenta guardar resultados en Parquet con Spark.
    - Si en Windows falla la escritura por dependencias nativas de Hadoop (NativeIO/winutils),
        aplica un fallback para guardar en Parquet usando pandas/pyarrow (manteniendo el artefacto final).


"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hour, avg, count, stddev, round as spark_round
import pandas as pd

def main():
    print("\n" + "=" * 70)
    print("üöÄ INICIANDO PROCESAMIENTO ESCALABLE CON PYSPARK")
    print("=" * 70)

    # 1Ô∏è‚É£ Crear sesi√≥n de Spark
    # Configuraci√≥n orientada a entornos Windows: reduce problemas de escritura Parquet
    # (el procesamiento y las agregaciones se ejecutan con Spark; el fallback s√≥lo aplica a la persistencia).
    spark = (SparkSession.builder
        .appName("Divvy Spark Processing")
        .config("spark.driver.memory", "4g")
        # Configuraciones para minimizar dependencias nativas en Windows
        .config("spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version", "2")
        .config("spark.hadoop.io.native.lib.available", "false")
        .config("spark.hadoop.native.lib", "false")
        .config("spark.hadoop.fs.file.impl", "org.apache.hadoop.fs.LocalFileSystem")
        .config("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MICROS")
        .getOrCreate())

    print("\n‚úÖ Sesi√≥n de Spark creada correctamente")
    print(f"   Versi√≥n de Spark: {spark.version}")

    # 2Ô∏è‚É£ Cargar dataset maestro
    # Entrada: Parquet consolidado por el pipeline (script 03), con clima y features base.
    parquet_path = "./data/processed/master_dataset.parquet"
    print(f"\nüìÇ Cargando dataset desde: {parquet_path}")

    df = spark.read.parquet(parquet_path)

    # 2Ô∏è‚É£1Ô∏è‚É£ Asegurar columnas temporales en tipo timestamp (por seguridad)
    # Las columnas ya fueron convertidas a microsegundos al escribir el Parquet en el script 03.
    # Si alguna llegara como otro tipo, se fuerza el cast.
    timestamp_cols = ["started_at", "ended_at", "datetime_hourly"]
    for col_name in timestamp_cols:
        if dict(df.dtypes).get(col_name) != 'timestamp':
            df = df.withColumn(col_name, col(col_name).cast("timestamp"))

    print(f"\n‚úÖ Dataset cargado con {df.count():,} filas y {len(df.columns)} columnas (timestamps listos)")

    # 3Ô∏è‚É£ Mostrar esquema
    print("\nüìã Esquema del dataset:")
    df.printSchema()

    # 4Ô∏è‚É£ Agregaci√≥n 1 (Spark): duraci√≥n promedio por hora
    # Crea `hour_of_day` desde `datetime_hourly` y calcula m√©tricas por franja horaria.
    print("\nüìä Calculando duraci√≥n promedio por hora del d√≠a...")
    df_hourly = (
        df.withColumn("hour_of_day", hour(col("datetime_hourly")))
          .groupBy("hour_of_day")
          .agg(
              spark_round(avg("trip_duration_minutes"), 2).alias("avg_trip_duration"),
              spark_round(stddev("trip_duration_minutes"), 2).alias("std_trip_duration"),
              count("*").alias("total_trips")
          )
          .orderBy("hour_of_day")
    )

    print("\nüïê Duraci√≥n promedio por hora:")
    df_hourly.show(24, truncate=False)

    # 5Ô∏è‚É£ Agregaci√≥n 2 (Spark): relaci√≥n temperatura-viajes (conteos por temperatura redondeada)
    if "temperature_2m" in df.columns:
        print("\nüå°Ô∏è Analizando relaci√≥n entre temperatura y cantidad de viajes...")
        df_temp = (
            df.groupBy(spark_round(col("temperature_2m"), 0).alias("temperature"))
              .agg(count("*").alias("num_trips"))
              .orderBy("temperature")
        )
        df_temp.show(20, truncate=False)
    else:
        print("\n‚ö†Ô∏è No se encontr√≥ columna de temperatura en el dataset")

    # 6Ô∏è‚É£ Persistencia: escribir resultados agregados
    # Se intenta primero con Spark (Parquet). Si falla en Windows por NativeIO/winutils,
    # se recurre a un fallback con pandas/pyarrow para garantizar el artefacto.
    output_dir = "D:/Nueva carpeta/Desktop/SPARKGT/data/processed/spark_aggregations.parquet"
    try:
        df_hourly.coalesce(1).write.mode("overwrite").parquet(output_dir)
        print(f"\nüíæ Resultados guardados en (Spark Parquet, carpeta): {output_dir}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Escritura Parquet con Spark fall√≥: {e}\n   ‚Üí Aplicando fallback con pandas/pyarrow...")
        # Fallback: guardar como archivo Parquet usando pandas/pyarrow
        fallback_file = "D:/Nueva carpeta/Desktop/SPARKGT/data/processed/spark_aggregations_pyarrow.parquet"
        pdf = df_hourly.toPandas()
        pdf.to_parquet(
            fallback_file,
            index=False,
            engine="pyarrow",
            compression="snappy",
            coerce_timestamps="us",
            allow_truncated_timestamps=True
        )
        print(f"   ‚úÖ Fallback guardado en: {fallback_file}")

    # Finalizar Spark
    spark.stop()
    print("\n‚úÖ Sesi√≥n de Spark finalizada correctamente")
    print("=" * 70)

if __name__ == "__main__":
    main()
